{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoT6ODFoVz8K"
      },
      "source": [
        "# Intel Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUw6Lm1ONoq4"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhaKLEACN-NH"
      },
      "outputs": [],
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgD05QX3N-Ox"
      },
      "outputs": [],
      "source": [
        "# Important imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import pickle\n",
        "import cv2\n",
        "import random\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import  LabelBinarizer\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, LeakyReLU\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzPXISMUN-Sq"
      },
      "outputs": [],
      "source": [
        "# Plotting 25 images to check dataset\n",
        "plt.figure(figsize=(11,11))\n",
        "path = \"/Intel Image Dataset/mountain\"\n",
        "for i in range(1,26):\n",
        "    plt.subplot(5,5,i)\n",
        "    plt.tight_layout()\n",
        "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
        "    plt.imshow(rand_img)\n",
        "    plt.title('mountain')\n",
        "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
        "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdyiHpahReF8"
      },
      "outputs": [],
      "source": [
        "# Setting root directory path and creating empty list\n",
        "dir = \"/Intel Image Dataset\"\n",
        "root_dir = listdir(dir)\n",
        "image_list, label_list = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLoajsN-RmQF"
      },
      "outputs": [],
      "source": [
        "# Reading and converting image to numpy array\n",
        "for directory in root_dir:\n",
        "  for files in listdir(f\"{dir}/{directory}\"):\n",
        "    image_path = f\"{dir}/{directory}/{files}\"\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((150,150)) # All images does not have same dimension\n",
        "    image = img_to_array(image)\n",
        "    image_list.append(image)\n",
        "    label_list.append(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX89xh5BRmlG"
      },
      "outputs": [],
      "source": [
        "# Visualize the number of classes count\n",
        "label_counts = pd.DataFrame(label_list).value_counts()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Pgx3XMRoaP"
      },
      "outputs": [],
      "source": [
        "# Checking count of classes\n",
        "num_classes = len(label_counts)\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJp5JRT0Rp7e"
      },
      "outputs": [],
      "source": [
        "# Checking x data shape\n",
        "np.array(image_list).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_aIkXkyRt2d"
      },
      "outputs": [],
      "source": [
        "# Checking y data shape\n",
        "label_list = np.array(label_list)\n",
        "label_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTfmSvoRt5l"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5GWA6z5Rxtm"
      },
      "outputs": [],
      "source": [
        "# Normalize and reshape data\n",
        "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
        "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
        "x_train = x_train.reshape( -1, 150,150,3)\n",
        "x_test = x_test.reshape( -1, 150,150,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ8NTYmfRzOS"
      },
      "outputs": [],
      "source": [
        "# Binarizing labels\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "print(lb.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JshCofPfR01i"
      },
      "outputs": [],
      "source": [
        "# Splitting the training data set into training and validation data sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN8XnsG9R2oA"
      },
      "outputs": [],
      "source": [
        "# Creating model architecture\n",
        "model = Sequential([\n",
        "        Conv2D(16, kernel_size = (3,3), input_shape = (150,150,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "          \n",
        "        Conv2D(32, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "        \n",
        "        Conv2D(64, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Conv2D(128, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "\n",
        "        Flatten(),\n",
        "    \n",
        "        Dense(64),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Dense(32),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "    \n",
        "        Dense(16),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(1),\n",
        "    \n",
        "        Dense(6, activation = 'softmax')    \n",
        "        ])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GvVgEiXR4sy"
      },
      "outputs": [],
      "source": [
        "# Compiling model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1I9HtY2R7R-"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "epochs = 70\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZBnscUwSDi5"
      },
      "outputs": [],
      "source": [
        "# Saving model\n",
        "pickle.dump(model, open('model.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBEz7EGGSFro"
      },
      "outputs": [],
      "source": [
        "#Plot the training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], color='r')\n",
        "plt.plot(history.history['val_accuracy'], color='b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auY2U-1USIk2"
      },
      "outputs": [],
      "source": [
        "#Plot the loss history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'], color='b')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRI9dX9eSI0j"
      },
      "outputs": [],
      "source": [
        "# Calculating test accuracy \n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_931pQbeSKyO"
      },
      "outputs": [],
      "source": [
        "# Storing model predictions\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNByJOaYSMNh"
      },
      "outputs": [],
      "source": [
        "# Plotting image to compare\n",
        "img = array_to_img(x_test[1])\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqAQkqooSQAo"
      },
      "outputs": [],
      "source": [
        "# Finding max value from predition list and comaparing original value vs predicted\n",
        "labels = lb.classes_\n",
        "print(labels)\n",
        "print(\"Originally : \",labels[np.argmax(y_test[1])])\n",
        "print(\"Predicted : \",labels[np.argmax(y_pred[1])])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Intel Image Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

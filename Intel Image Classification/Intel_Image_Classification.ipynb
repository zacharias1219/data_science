{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoT6ODFoVz8K"
      },
      "source": [
        "# Intel Image Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhWtJlWV7m6"
      },
      "source": [
        "In this project, we will be working on Intel images i.e. images of buildings, forest, street, etc. We will built a convolutional neural network and train it on this images. This is a multi class classification problem and we will use Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUw6Lm1ONoq4"
      },
      "outputs": [],
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53m7mK_1yxb"
      },
      "source": [
        "First we will mount our google drive on colab so that we can use the dataset directly from our drive. For this you first need to upload the data on your drive and then mount the drive on colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhaKLEACN-NH"
      },
      "outputs": [],
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCkhamd_11rC"
      },
      "source": [
        "After mounting our drive we will locate the folder where our data is stored to use it in our colab notebook. Here we will see all the folders I have in my drive and 'Intel Image Dataset' contains the images that we will work on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgD05QX3N-Ox"
      },
      "outputs": [],
      "source": [
        "# Important imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import cv2\n",
        "import random\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import  LabelBinarizer\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, LeakyReLU\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cSEn4h01__w"
      },
      "source": [
        "We will start by importing some required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxImdzDnRWQn"
      },
      "outputs": [],
      "source": [
        "# Listing directory\n",
        "!ls \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBh7ypts2DQ0"
      },
      "source": [
        "We will check for folders of class images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzPXISMUN-Sq"
      },
      "outputs": [],
      "source": [
        "# Plotting 25 images to check dataset\n",
        "plt.figure(figsize=(11,11))\n",
        "path = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset/mountain\"\n",
        "for i in range(1,26):\n",
        "    plt.subplot(5,5,i)\n",
        "    plt.tight_layout()\n",
        "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
        "    plt.imshow(rand_img)\n",
        "    plt.title('mountain')\n",
        "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
        "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puab6nG92K2k"
      },
      "source": [
        "Let's visualize some of the mountain images that we will be working on. Also we will observe x and y dimensions of the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdyiHpahReF8"
      },
      "outputs": [],
      "source": [
        "# Setting root directory path and creating empty list\n",
        "dir = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset\"\n",
        "root_dir = listdir(dir)\n",
        "image_list, label_list = [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgcSBgpU2T_z"
      },
      "source": [
        "Setting the root directory for the dataset and storing all the folders name of the dataset. We will also create 2 empty list for image and lables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLoajsN-RmQF"
      },
      "outputs": [],
      "source": [
        "# Reading and converting image to numpy array\n",
        "for directory in root_dir:\n",
        "  for files in listdir(f\"{dir}/{directory}\"):\n",
        "    image_path = f\"{dir}/{directory}/{files}\"\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((150,150)) # All images does not have same dimension\n",
        "    image = img_to_array(image)\n",
        "    image_list.append(image)\n",
        "    label_list.append(directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGe05NlN2X5B"
      },
      "source": [
        "Next we need to resize images as some of the images don't have same dimensions. So, we will read and resize all the images. Then we will convert it into array and appending the list created above with the image and its label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX89xh5BRmlG"
      },
      "outputs": [],
      "source": [
        "# Visualize the number of classes count\n",
        "label_counts = pd.DataFrame(label_list).value_counts()\n",
        "label_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSQA5G8L57wY"
      },
      "source": [
        "Checking for images per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Pgx3XMRoaP"
      },
      "outputs": [],
      "source": [
        "# Checking count of classes\n",
        "num_classes = len(label_counts)\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhRcDCf5-eN"
      },
      "source": [
        "Storing the number of classes which will be used further in model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJp5JRT0Rp7e"
      },
      "outputs": [],
      "source": [
        "# Checking x data shape\n",
        "np.array(image_list).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5Lol8U6Shy"
      },
      "source": [
        "Check the shape of the x data for input layer of model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_aIkXkyRt2d"
      },
      "outputs": [],
      "source": [
        "# Checking y data shape\n",
        "label_list = np.array(label_list)\n",
        "label_list.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qs2_pO16fxA"
      },
      "source": [
        "Checking the number of labels in y data which should be equal to total number of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTfmSvoRt5l"
      },
      "outputs": [],
      "source": [
        "# Splitting dataset into test and train\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2n-QTR65g1"
      },
      "source": [
        "Now we will split our dataset into testing and training using train_test_split() from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5GWA6z5Rxtm"
      },
      "outputs": [],
      "source": [
        "# Normalize and reshape data\n",
        "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
        "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
        "x_train = x_train.reshape( -1, 150,150,3)\n",
        "x_test = x_test.reshape( -1, 150,150,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxub0cgO7EzH"
      },
      "source": [
        "Next we will normalize the images by dividing them with 255 and we will also reshape x_train and x_test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ8NTYmfRzOS"
      },
      "outputs": [],
      "source": [
        "# Binarizing labels\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "print(lb.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuVKsoqF7XK_"
      },
      "source": [
        "Here we will use label binarizer to one hot encode our y data. We will also print the sequence of the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JshCofPfR01i"
      },
      "outputs": [],
      "source": [
        "# Splitting the training data set into training and validation data sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34xAE2Nz7iEo"
      },
      "source": [
        "Now we will split the training data to validation and training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN8XnsG9R2oA"
      },
      "outputs": [],
      "source": [
        "# Creating model architecture\n",
        "model = Sequential([\n",
        "        Conv2D(16, kernel_size = (3,3), input_shape = (150,150,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "          \n",
        "        Conv2D(32, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "        \n",
        "        Conv2D(64, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Conv2D(128, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "\n",
        "        Flatten(),\n",
        "    \n",
        "        Dense(64),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Dense(32),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "    \n",
        "        Dense(16),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(1),\n",
        "    \n",
        "        Dense(6, activation = 'softmax')    \n",
        "        ])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyAv8tUF72vw"
      },
      "source": [
        "Now we will create a network architecture for the model. We have used different types of layers according to their features namely BatchNormalization (Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each mini-batch), LeakyRelu (The Leaky ReLU modifies the function to allow small negative values when the input is less than zero), Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel). In the last layer we will use softmax as the activation function because it is a multi class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GvVgEiXR4sy"
      },
      "outputs": [],
      "source": [
        "# Compiling model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Uhzshq8bci"
      },
      "source": [
        "For compiling the model we need to pass 3 parameters namely loss, optimizer and metrics. Here we will use loss as categorical_crossentropy, optimizer as Adam and metrics as accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1I9HtY2R7R-"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "epochs = 70\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJ2AIYK8y2t"
      },
      "source": [
        "Fitting the model with the data and finding out the accuracy at each epoch to see how our model is learning. Now we will train our model on 70 epochs and a batch size of 128. You can try using more number of epochs to increase accuracy. During each epochs we can see how the model is performing by viewing the training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZBnscUwSDi5"
      },
      "outputs": [],
      "source": [
        "# Saving model\n",
        "model.save(\"/content/drive/My Drive/intel_image.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baX1EiUc9LnF"
      },
      "source": [
        "We will save the model using model.save() to use it later for prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBEz7EGGSFro"
      },
      "outputs": [],
      "source": [
        "#Plot the training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], color='r')\n",
        "plt.plot(history.history['val_accuracy'], color='b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQaRBDO9VEJ"
      },
      "source": [
        "Next we will plot the accuracy of the model for the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auY2U-1USIk2"
      },
      "outputs": [],
      "source": [
        "#Plot the loss history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'], color='b')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdJ074B9bqu"
      },
      "source": [
        "Next we will plot the loss of the model for the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRI9dX9eSI0j"
      },
      "outputs": [],
      "source": [
        "# Calculating test accuracy \n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiGjHNML9f_B"
      },
      "source": [
        "Evaluating the model to know the accuracy of the model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_931pQbeSKyO"
      },
      "outputs": [],
      "source": [
        "# Storing model predictions\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4MrMF8Q9meA"
      },
      "source": [
        "Generating predictions for test data and storing them into y_pred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNByJOaYSMNh"
      },
      "outputs": [],
      "source": [
        "# Plotting image to compare\n",
        "img = array_to_img(x_test[1])\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdmALWF9rYp"
      },
      "source": [
        "Visualizing an image to be predicted in further steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqAQkqooSQAo"
      },
      "outputs": [],
      "source": [
        "# Finding max value from predition list and comaparing original value vs predicted\n",
        "labels = lb.classes_\n",
        "print(labels)\n",
        "print(\"Originally : \",labels[np.argmax(y_test[1])])\n",
        "print(\"Predicted : \",labels[np.argmax(y_pred[1])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EreThly90vE"
      },
      "source": [
        "Now, we will create list of labels using object of label binarizer. We will print that list and finally we will print out the prediction and the original label of the image we visualized above using argmax()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0ndQ_o5V_r2"
      },
      "source": [
        "## Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FXJgeq7WDBg"
      },
      "source": [
        "In this project we saw how we can create a CNN using different layers. Normalizing is an important step when working with any type of dataset. We will use this model to predict the class of the image supplied to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni7-suUgWF-Z"
      },
      "source": [
        "## Scope:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIaqIL-3-sKo"
      },
      "source": [
        "This project has a vast scope, it can be used to classify satellite images, drone images, google images into different classes like sea, mountain, etc. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3gqSpVKSSx3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Intel Image Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
